## Техническое видение проекта

### Технологии

- **Язык и рантайм**: Python 3.11.
- **Управление окружением и зависимостями**: `uv` как основной инструмент для изоляции окружения и установки зависимостей.
- **Интерфейс MVP**: простое CLI-приложение на Python без веб-фреймворков. Взаимодействие с пользователем через командную строку.
- **Работа с LLM**: официальный `openai`-клиент, настроенный на использование OpenRouter (через базовый URL и API-ключ).
- **Хранение состояния**: история диалога и вспомогательные данные хранятся в памяти процесса в виде простых структур Python (`dict`, `list`). Базы данных не используются.
- **Тестирование**: `pytest` для написания и запуска тестов.
- **Автоматизация**: `Makefile` с минимальным набором команд (`make run`, `make test`, при необходимости дополнительные простые цели).
- **Контейнеризация и окружение выполнения**: Docker-контейнер на базе образа Linux (`python:3.11-slim`) с установкой `uv` и зависимостей проекта.
- **Дополнительные инструменты**: на MVP-этапе не используются дополнительные фреймворки и тяжёлые библиотеки. Приоритет — минимальный стек и принцип KISS.

### Принцип разработки

- **Фокус на MVP**: реализуем только минимально необходимый функционал для проверки продуктовой идеи (ключевые сценарии работы Telegram-бота-ассистента).
- **Итеративность**: двигаемся маленькими инкрементами, каждый шаг — это простой, но работающий сценарий.
- **Минимальная модульность**: код разделяется на небольшое количество модулей по очевидным зонам ответственности (ядро приложения, клиент LLM, сценарии, утилиты), без усложнения архитектуры.
- **Минимум абстракций**: избегаем сложных паттернов и универсальных фреймворков. Взаимодействие с LLM реализуется через простой модуль-клиент, который можно напрямую вызывать из сценариев.
- **Тесты только для важного**: пишем тесты для критически важной логики (формирование промптов, обработка ответов, выбор сценариев и базовая оркестрация), без требования полного покрытия.
- **Простота принятия решений**: все технические решения принимаются с приоритетом простоты и скорости разработки. Если есть сомнения, выбираем самый простой вариант, который можно быстро изменить позже.

### Структура проекта

- **Корень репозитория**
  - `README.md` — краткое описание проекта и инструкции по запуску.
  - `doc/` — документация проекта (`product_idea.md`, `vision.md` и другие текстовые материалы).
  - `Makefile` — простые команды для запуска приложения, тестов и вспомогательных действий.
  - `pyproject.toml`/конфигурация `uv` — описание зависимостей и настроек проекта.
  - `Dockerfile` — описание Docker-образа для развёртывания.
- **Исходный код**
  - `src/llm_cursor/`
    - `__init__.py` — инициализация пакета.
    - `cli.py` — входная точка CLI-приложения.
    - `app.py` — основная оркестрация сценариев и работы приложения.
    - `llm_client.py` — простой модуль-клиент для работы с LLM через OpenRouter.
    - `scenarios.py` — описание сценариев работы ассистента.
    - `telegram_bot.py` — модуль для интеграции с Telegram (может быть реализован позже, но зарезервирован в структуре).
    - `config.py` — простая конфигурация приложения (чтение переменных окружения, базовые константы).
- **Тесты**
  - `tests/`
    - `test_app.py` — тесты основной логики приложения.
    - `test_llm_client.py` — тесты взаимодействия с модулем клиента LLM (с использованием подмен/фиктивных ответов).
    - при необходимости — дополнительные тесты для ключевых сценариев.

### Архитектура проекта

- **Общий подход**
  - Архитектура описывается как простой набор модулей и потоков вызовов без строгого деления на слои и сложных паттернов.
  - Основная цель — прозрачный и легко изменяемый код, достаточный для MVP.

- **Поток через CLI**
  - Пользователь запускает CLI-команду.
  - `cli.py` парсит аргументы/команды и вызывает высокоуровневые функции из `app.py`.
  - `app.py`:
    - инициализирует базовую конфигурацию и состояние (историю диалога);
    - выбирает подходящий сценарий из `scenarios.py`;
    - обращается к `llm_client.py` для получения ответа от LLM;
    - обновляет историю и формирует ответ для вывода в консоль.

- **Работа с LLM**
  - `llm_client.py` содержит минимальный API для взаимодействия с OpenRouter через `openai`:
    - инициализацию клиента (с учётом базового URL и токена из переменных окружения);
    - одну-две основные функции для отправки промпта и получения ответа.
  - Формирование промпта и управление историей диалога выполняется на уровне `app.py`/`scenarios.py`, а не внутри клиента.

- **Сценарии**
  - `scenarios.py` описывает ключевые сценарии использования ассистента (например, консультация по определённой теме).
  - Каждый сценарий:
    - принимает вход от пользователя (текст/контекст);
    - при необходимости модифицирует или дополняет промпт;
    - вызывает `llm_client` и обрабатывает ответ;
    - возвращает результат в удобном для `app.py` виде.

- **История и состояние**
  - История диалога хранится в простых структурах Python (`list` словарей/сообщений).
  - Состояние (история, текущий сценарий и т.п.) передаётся между функциями явно через аргументы и возвращаемые значения.
  - На уровне MVP не используется внешнее хранилище; всё состояние живёт в памяти процесса.

- **Интеграция с Telegram**
  - `telegram_bot.py` реализует адаптер между Telegram и основной логикой приложения:
    - при получении сообщения от Telegram оно преобразуется в унифицированный «вход пользователя» (текст + простой контекст);
    - адаптер вызывает функции `app.py` и сценарии из `scenarios.py` так же, как это делает CLI;
    - ответ от LLM преобразуется в формат сообщения для Telegram и отправляется пользователю.
  - Транспортный слой (polling/webhook) реализуется максимально просто:
    - на MVP-этапе допускается использование long polling;
    - детали конфигурации (токен бота, режим работы) задаются через переменные окружения и модуль `config.py`.

- **Обработка ошибок**
  - На уровне MVP реализуется базовая обработка ошибок:
    - перехват ключевых исключений (проблемы с сетью, некорректная конфигурация);
    - вывод понятного сообщения пользователю (в CLI или в Telegram);
    - простое логирование ошибок для последующего анализа.
  - Сложные механизмы ретраев, метрик и распределённого трейсинга на этапе MVP не используются.

### Модель данных

- **Сообщение диалога**
  - Представляется структурой, максимально близкой к формату `openai`:
    - `role`: `"system" | "user" | "assistant"`;
    - `content`: строка с текстом сообщения;
    - опционально `metadata`: словарь с простыми дополнительными флагами/полями (например, источник сообщения).

- **История диалога**
  - Список сообщений: `List[Message]` в виде обычного списка словарей/объектов.
  - История передаётся в LLM без лишних преобразований, чтобы упростить код клиента.

- **Контекст пользователя (Telegram)**
  - Минимальный набор полей:
    - `user_id` — идентификатор пользователя Telegram;
    - `chat_id` — идентификатор чата;
    - `metadata` — произвольный словарь для хранения простых дополнительных данных (например, текущий сценарий или шаг сценария).
  - Контекст используется для выбора сценария и понимания того, как отвечать пользователю, без сохранения в базу данных.

- **Конфигурация LLM**
  - Простая структура (словарь или dataclass) с ключевыми параметрами:
    - `model` — идентификатор модели;
    - `temperature`, `max_tokens` и другие базовые параметры генерации.
  - Конфигурация передаётся в функции клиента LLM по мере необходимости.

- **Результат сценария**
  - Простая структура с основным полем `reply_text` (ответ ассистента в виде строки).
  - При необходимости может содержать дополнительные простые поля (например, флаг «завершить диалог»).

- **Сохранность данных**
  - Вся модель данных живёт в памяти процесса (структуры Python).
  - История диалога и контекст пользователей не сохраняются между перезапусками приложения; на уровне MVP данные считаются эфемерными.

### Работа с LLM

- **Модуль клиента**
  - В модуле `llm_client.py` реализуется минимальный клиент для работы с OpenRouter через библиотеку `openai`.
  - Клиент инициализируется с использованием:
    - базового URL OpenRouter;
    - API-ключа, получаемого из переменных окружения (через `config.py`).

- **Конфигурация модели**
  - Идентификатор модели и параметры генерации (`temperature`, `max_tokens` и др.) задаются через конфигурацию приложения.
  - Конфигурация передаётся в функции клиента явно, без жёстко зашитых значений в коде клиента.

- **Формирование запроса**
  - В клиент передаётся список сообщений в формате, описанном в модели данных (как у `openai`).
  - Перед отправкой запроса к истории добавляется один системный `system`-месседж с базовой ролью ассистента (описание того, что это Telegram-бот-ассистент для консультаций и как он должен отвечать).

- **Получение и обработка ответа**
  - Клиент выполняет один простой чат-запрос без стриминга и сложных дополнений (инструменты, функции и т.п. не используются).
  - Из ответа берётся первое сгенерированное сообщение (`choices[0].message.content`), которое далее передаётся в сценарий и пользователю.

- **Обработка ошибок**
  - В случае ошибок при обращении к LLM:
    - ошибка логируется с минимальным набором деталей (тип, сообщение);
    - пользователю возвращается единое понятное fallback-сообщение («Сервис временно недоступен, попробуйте позже» или аналогичное).
  - Дополнительные механизмы (ретраи, сложные стратегии деградации) в рамках MVP не применяются.

### Мониторинг LLM

- **Каналы логирования**
  - Основной канал логирования — стандартный вывод (stdout). В Docker-окружении это позволяет собирать логи без дополнительной настройки.
  - Отдельные файловые логи на этапе MVP не используются, чтобы не усложнять развёртывание и конфигурацию.

- **Что логируем**
  - Для каждого запроса к LLM логируется:
    - время запроса;
    - идентификатор модели;
    - длительность выполнения;
    - статус выполнения (успех/ошибка).
  - В обычном режиме текст промпта и ответа полностью не логируется, чтобы избежать излишнего шума и потенциальных рисков по данным.

- **Отладочный режим**
  - В конфигурации предусмотрен флаг (например, `DEBUG_LLM`), который включает расширенное логирование.
  - В отладочном режиме могут логироваться усечённые или полные версии промптов и ответов для упрощения диагностики.

- **Приватность и данные пользователей**
  - В логах не должно быть явных персональных данных пользователей, если этого можно избежать.
  - Любые потенциально чувствительные данные по возможности не логируются или логируются в обезличенном/усечённом виде, особенно в обычном (неотладочном) режиме.

### Сценарии работы

- **Основной сценарий (консультация)**
  - Единственный ключевой сценарий MVP — консультация по вопросам пользователя.
  - Пользователь формулирует запрос (в Telegram или через CLI), ассистент отвечает, опираясь на историю диалога и системный промпт.
  - Вся логика сценария сосредоточена в `scenarios.py` и использует общий клиент LLM и модель данных.

- **Команды Telegram**
  - `/start`:
    - отправляет короткую справку о боте и о том, как формулировать запросы;
    - после отображения справки переводит пользователя в обычный консультационный сценарий (первый вопрос воспринимается как начало диалога).
  - `/help`:
    - выводит список доступных команд и краткие рекомендации по использованию бота;
    - не меняет внутреннее состояние сценария, работает как справочная команда.
  - `/reset`:
    - очищает историю диалога для текущего пользователя/чата (сбрасывает хранимые в памяти сообщения и контекст);
    - после сброса следующий запрос воспринимается как начало нового диалога.

- **Единый механизм для CLI и Telegram**
  - CLI использует тот же сценарий консультации, что и Telegram, меняется только способ ввода/вывода.
  - Вся логика выбора сценария и обработки команд сосредоточена в `scenarios.py` и вызывается как из `app.py` (CLI), так и из `telegram_bot.py`.

### Деплой

- **Общий подход**
  - Приложение разворачивается в виде одного Docker-контейнера без использования оркестраторов (docker-compose, Kubernetes и т.п.).
  - Один и тот же образ используется и для CLI-режима, и для запуска Telegram-бота; режим работы выбирается командой запуска.

- **Docker-образ**
  - Базовый образ: `python:3.11-slim`.
  - На этапе сборки:
    - устанавливается `uv`;
    - копируется исходный код проекта и конфигурационные файлы;
    - через `uv` устанавливаются зависимости.
  - Рабочий каталог и переменные окружения настраиваются минимально, без лишних оптимизаций.

- **Режимы запуска**
  - **CLI-режим (по умолчанию)**:
    - точка входа — CLI-команда, определённая в `cli.py` (например, `python -m llm_cursor.cli` или аналогичный скрипт);
    - используется для локального тестирования и отладки.
  - **Режим Telegram-бота**:
    - запускается той же командой `docker run`, но с другой командой контейнера (например, `python -m llm_cursor.telegram_bot`);
    - логика бота использует те же сценарии и клиента LLM, что и CLI.

- **Переменные окружения**
  - Основные переменные окружения, необходимые для работы контейнера:
    - `OPENROUTER_API_KEY` — ключ доступа к OpenRouter;
    - `OPENROUTER_BASE_URL` — базовый URL для API OpenRouter (если требуется);
    - `TELEGRAM_BOT_TOKEN` — токен Telegram-бота (нужен только в режиме Telegram);
    - дополнительные переменные конфигурации (например, `LLM_MODEL`, `LLM_TEMPERATURE`, `DEBUG_LLM`) по мере необходимости.
  - Переменные окружения передаются в контейнер при запуске (`docker run -e ...`), без использования сложных конфигурационных механизмов.

- **Развёртывание**
  - Базовый сценарий деплоя:
    - собрать образ (`docker build ...`);
    - запустить контейнер на целевой машине (`docker run ...`) с нужными переменными окружения.
  - Никаких дополнительных компонентов (отдельных БД, брокеров сообщений и т.п.) на этапе MVP не используется.

### Подход к конфигурированию

- **Источник конфигурации**
  - Основным источником конфигурации являются переменные окружения.
  - Допускается использование простых констант в коде для значений по умолчанию (например, дефолтная модель или температура), но без отдельных файлов конфигурации (`.yaml`, `.json` и т.п.) на уровне MVP.

- **Модуль `config.py`**
  - Отвечает за чтение и валидацию параметров окружения.
  - Содержит простые функции:
    - `get_llm_config()` — возвращает словарь с настройками LLM (модель, температура, максимальное число токенов и т.п.), подставляя значения по умолчанию при отсутствии переменных окружения.
    - `get_telegram_config()` — возвращает настройки Telegram-бота (токен, режим работы и др.).
    - `get_flags()` — возвращает флаги/режимы (например, `DEBUG_LLM` и другие простые булевы опции).

- **Обработка отсутствующих критических настроек**
  - Если при инициализации приложения обнаруживается отсутствие критически важной переменной (например, `OPENROUTER_API_KEY` или `TELEGRAM_BOT_TOKEN` при запуске бота), код не просто падает с трассировкой.
  - Вместо этого:
    - в логи выводится понятное описание проблемы (какая переменная не задана и зачем она нужна);
    - пользователю (в CLI или в Telegram, если это возможно) возвращается простое и содержательное сообщение с просьбой проверить настройки.
  - Такой подход сохраняет простоту реализации и облегчает диагностику проблем на этапе MVP.

### Подход к логгированию

- **Инструменты**
  - Используется стандартный модуль Python `logging` без дополнительных библиотек и форматов.
  - Логи выводятся в стандартный поток вывода (stdout), чтобы их можно было собирать средствами Docker и инфраструктуры.

- **Формат и уровни**
  - Простой текстовый формат сообщений (время, уровень, модуль, сообщение) без структурированных/JSON-логов.
  - Основные уровни:
    - `INFO` — ключевые события и рабочий поток (запуск приложения, обработка запросов, применение команд `/start`, `/reset` и т.п.);
    - `ERROR` — ошибки при работе с LLM, конфигурацией, Telegram и другими внешними системами;
    - `DEBUG` — детальная служебная информация, включаемая только при отладке.
  - Уровень логирования по умолчанию — `INFO`. Включение `DEBUG`-уровня происходит через конфигурационный флаг (например, `DEBUG_LLM` или общий `DEBUG`).

- **Ключевые точки логирования**
  - Старт и завершение приложения (для CLI и Telegram-режима).
  - Ошибки при инициализации конфигурации (отсутствующие переменные окружения и т.п.).
  - Ошибки при взаимодействии с LLM (с учётом требований по приватности данных).
  - Ошибки/исключения при работе Telegram-бота (например, проблемы с сетью или API Telegram).
  - Выполнение критичных команд, влияющих на состояние диалога (например, `/reset`).





